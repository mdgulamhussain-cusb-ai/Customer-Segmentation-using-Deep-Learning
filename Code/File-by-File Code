# src/config.py
DATA_PATH = "data/raw/customer_data.csv"
PROCESSED_DATA_PATH = "data/processed/segmented_customers.csv"

ENCODING_DIM = 8
N_CLUSTERS = 4

EPOCHS = 50
BATCH_SIZE = 32
RANDOM_STATE = 42


# src/data_loader.py
import pandas as pd
from src.config import DATA_PATH

def load_data():
    return pd.read_csv(DATA_PATH)


# src/preprocessing.py
from sklearn.preprocessing import StandardScaler

def preprocess_data(df):
    numeric_df = df.select_dtypes(include=["int64", "float64"])
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(numeric_df)
    
    return numeric_df, scaled_data


# src/autoencoder.py
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

def build_autoencoder(input_dim, encoding_dim):
    input_layer = Input(shape=(input_dim,))
    
    encoded = Dense(16, activation="relu")(input_layer)
    encoded = Dense(encoding_dim, activation="relu")(encoded)

    decoded = Dense(16, activation="relu")(encoded)
    decoded = Dense(input_dim, activation="linear")(decoded)

    autoencoder = Model(inputs=input_layer, outputs=decoded)
    encoder = Model(inputs=input_layer, outputs=encoded)

    autoencoder.compile(
        optimizer=Adam(learning_rate=0.001),
        loss="mse"
    )
    
    return autoencoder, encoder


